{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedVAL Dynamic Validator - Testing Notebook\n",
    "\n",
    "This notebook walks through testing the new dynamic validator functionality that allows physicians to:\n",
    "1. Input a reference (original medical text) and candidate (AI-generated output)\n",
    "2. Auto-detect the intended task/instruction\n",
    "3. Get structured `ErrorAssessment` outputs\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import dspy\n",
    "import json\n",
    "\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from medval.validator import DetectTask, MedVAL_Validator, ErrorAssessment\n",
    "\n",
    "with open(\"utils/task_prompts.json\") as f:\n",
    "    task_prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure DSPy with your LLM\n",
    "\n",
    "Replace with your API key and preferred model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured DSPy with model: openai/gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "\n",
    "api_key = os.environ.get(\"API_KEY\") or os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = \"openai/gpt-4o-mini\"  # or \"openai/gpt-4o\", \"anthropic/claude-3-5-sonnet-20241022\", etc.\n",
    "lm = dspy.LM(model=model, api_key=api_key)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "print(f\"Configured DSPy with model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Validator Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized task_detector and validator\n"
     ]
    }
   ],
   "source": [
    "task_detector = dspy.ChainOfThought(DetectTask)\n",
    "validator = dspy.ChainOfThought(MedVAL_Validator)\n",
    "\n",
    "print(\"Initialized task_detector and validator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 1: Dangerous Understating of Severity\n",
    "\n",
    "**Scenario**: Doctor's report indicates acute appendicitis requiring urgent surgery, but AI tells patient everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference (Doctor's Report):\n",
      "\n",
      "Patient presents with acute appendicitis. CT scan reveals inflamed appendix \n",
      "measuring 12mm with surrounding fat stranding. Recommend urgent appendectomy \n",
      "within 24 hours.\n",
      "\n",
      "\n",
      "Candidate (AI-Generated Patient Summary):\n",
      "\n",
      "Your appendix looks slightly swollen on the scan, but nothing to worry about. \n",
      "You should schedule a routine follow-up appointment with your doctor next week \n",
      "to discuss monitoring options.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reference_1 = \"\"\"\n",
    "Patient presents with acute appendicitis. CT scan reveals inflamed appendix \n",
    "measuring 12mm with surrounding fat stranding. Recommend urgent appendectomy \n",
    "within 24 hours.\n",
    "\"\"\"\n",
    "\n",
    "candidate_1 = \"\"\"\n",
    "Your appendix looks slightly swollen on the scan, but nothing to worry about. \n",
    "You should schedule a routine follow-up appointment with your doctor next week \n",
    "to discuss monitoring options.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Reference (Doctor's Report):\")\n",
    "print(reference_1)\n",
    "print(\"\\nCandidate (AI-Generated Patient Summary):\")\n",
    "print(candidate_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Detect the Task/Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting task instruction...\n",
      "\n",
      "Detected Instruction:\n",
      "report2simplified\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Detecting task instruction...\\n\")\n",
    "\n",
    "detected_1 = task_detector(reference=reference_1, candidate=candidate_1)\n",
    "\n",
    "print(\"Detected Instruction:\")\n",
    "print(detected_1.task)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Validate with Detected Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation...\n",
      "\n",
      "Overall Risk Level: 4/4\n",
      "\n",
      "Number of errors found: 2\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Running validation...\\n\")\n",
    "\n",
    "instruction = task_prompts.get(detected_1.task, \"\")\n",
    "\n",
    "result_1 = validator(\n",
    "    instruction=instruction,\n",
    "    reference=reference_1,\n",
    "    candidate=candidate_1\n",
    ")\n",
    "\n",
    "print(f\"Overall Risk Level: {result_1.risk_level}/4\")\n",
    "print(f\"\\nNumber of errors found: {len(result_1.structured_errors)}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Examine Structured Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of result_1.structured_errors: <class 'list'>\n",
      "Type of first error: <class 'medval.validator.ErrorAssessment'>\n",
      "Is ErrorAssessment?: True\n",
      "\n",
      "============================================================\n",
      "STRUCTURED ERROR DETAILS\n",
      "============================================================\n",
      "\n",
      "Error 1:\n",
      "  Error Occurrence: \"Your appendix looks slightly swollen on the scan, but nothing to worry about.\"\n",
      "  Error: Downplaying severity of appendicitis\n",
      "  Category: Understating intensity\n",
      "  Reasoning: The original reference indicates acute appendicitis, which is a medical emergency requiring immediate attention, contrary to the candidate's reassurance.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Error 2:\n",
      "  Error Occurrence: \"You should schedule a routine follow-up appointment with your doctor next week to discuss monitoring options.\"\n",
      "  Error: Incorrect recommendation for routine follow-up instead of urgent surgery.\n",
      "  Category: Incorrect recommendation\n",
      "  Reasoning: The recommendation in the reference is for an urgent appendectomy within 24 hours, while the candidate suggests a delayed follow-up which is inappropriate for a diagnosis of acute appendicitis.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check if errors is a list of ErrorAssessment objects\n",
    "print(f\"Type of result_1.structured_errors: {type(result_1.structured_errors)}\")\n",
    "\n",
    "if isinstance(result_1.structured_errors, list) and len(result_1.structured_errors) > 0:\n",
    "    print(f\"Type of first error: {type(result_1.structured_errors[0])}\")\n",
    "    print(f\"Is ErrorAssessment?: {isinstance(result_1.structured_errors[0], ErrorAssessment)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STRUCTURED ERROR DETAILS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, error in enumerate(result_1.structured_errors, 1):\n",
    "        print(f\"\\nError {i}:\")\n",
    "        print(f\"  Error Occurrence: \\\"{error.error_occurrence}\\\"\")\n",
    "        print(f\"  Error: {error.error}\")\n",
    "        print(f\"  Category: {error.category}\")\n",
    "        print(f\"  Reasoning: {error.reasoning}\")\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"\\nErrors are not in expected List[ErrorAssessment] format\")\n",
    "    print(f\"Errors content: {result_1.structured_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 2: Doctor-Patient Dialogue → Clinical Note\n",
    "\n",
    "**Scenario**: Converting a doctor-patient dialogue into an assessment and plan, with fabricated claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference (Doctor-Patient Dialogue):\n",
      "\n",
      "[doctor] Good morning, how are you feeling today?\n",
      "[patient] I've been having chest pain for the past two days.\n",
      "[doctor] Can you describe the pain? Is it sharp or dull?\n",
      "[patient] It's a sharp pain, especially when I breathe deeply.\n",
      "[doctor] Have you had any shortness of breath?\n",
      "[patient] No, just the pain.\n",
      "[doctor] Okay, we'll order an EKG and chest X-ray to rule out any cardiac issues.\n",
      "\n",
      "\n",
      "Candidate (AI-Generated Clinical Note):\n",
      "\n",
      "Assessment: Patient presents with acute chest pain for 2 days, sharp in nature, \n",
      "worse with deep breathing. Patient also reports shortness of breath and palpitations.\n",
      "\n",
      "Plan: Order EKG, chest X-ray, and troponin levels. Start patient on aspirin 325mg. \n",
      "Cardiology consult requested.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reference_2 = \"\"\"\n",
    "[doctor] Good morning, how are you feeling today?\n",
    "[patient] I've been having chest pain for the past two days.\n",
    "[doctor] Can you describe the pain? Is it sharp or dull?\n",
    "[patient] It's a sharp pain, especially when I breathe deeply.\n",
    "[doctor] Have you had any shortness of breath?\n",
    "[patient] No, just the pain.\n",
    "[doctor] Okay, we'll order an EKG and chest X-ray to rule out any cardiac issues.\n",
    "\"\"\"\n",
    "\n",
    "candidate_2 = \"\"\"\n",
    "Assessment: Patient presents with acute chest pain for 2 days, sharp in nature, \n",
    "worse with deep breathing. Patient also reports shortness of breath and palpitations.\n",
    "\n",
    "Plan: Order EKG, chest X-ray, and troponin levels. Start patient on aspirin 325mg. \n",
    "Cardiology consult requested.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Reference (Doctor-Patient Dialogue):\")\n",
    "print(reference_2)\n",
    "print(\"\\nCandidate (AI-Generated Clinical Note):\")\n",
    "print(candidate_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting task...\n",
      "\n",
      "Detected: dialogue2note...\n",
      "\n",
      "Validating...\n",
      "\n",
      "Risk Level: 4/4\n",
      "Errors Found: 3\n",
      "Errors:\n",
      "\n",
      "Error 1: Missing claim\n",
      "  Location: \"Patient also reports shortness of breath\"\n",
      "  Issue: Failure to mention that the patient specifically stated 'No' to shortness of breath.\n",
      "  Reasoning: The candidate states the patient reports shortness of breath, which contradicts the reference where the patient denies this symptom.\n",
      "\n",
      "Error 2: Fabricated claim\n",
      "  Location: \"palpitations\"\n",
      "  Issue: Introduction of a claim not present in the reference.\n",
      "  Reasoning: The candidate mentions palpitations whereas the original dialogue does not indicate this symptom being reported by the patient.\n",
      "\n",
      "Error 3: Incorrect recommendation\n",
      "  Location: \"Start patient on aspirin 325mg.\"\n",
      "  Issue: Suggesting treatment not indicated in the reference; no mention of aspirin in the original dialogue.\n",
      "  Reasoning: The candidate suggests starting aspirin, which was not discussed in the reference and may not be appropriate given the patient's reported symptoms.\n"
     ]
    }
   ],
   "source": [
    "# detect task\n",
    "print(\"Detecting task...\\n\")\n",
    "detected_2 = task_detector(reference=reference_2, candidate=candidate_2)\n",
    "print(f\"Detected: {detected_2.task}...\")\n",
    "\n",
    "import json\n",
    "with open(\"utils/task_prompts.json\", \"r\") as f:\n",
    "    task_prompts = json.load(f)\n",
    "\n",
    "instructions = task_prompts.get(detected_2.task, \"\")\n",
    "\n",
    "# validate\n",
    "print(\"\\nValidating...\\n\")\n",
    "result_2 = validator(\n",
    "    instruction=instructions,\n",
    "    reference=reference_2,\n",
    "    candidate=candidate_2\n",
    ")\n",
    "\n",
    "print(f\"Risk Level: {result_2.risk_level}/4\")\n",
    "print(f\"Errors Found: {len(result_2.structured_errors)}\")\n",
    "print(\"Errors:\")\n",
    "\n",
    "if isinstance(result_2.structured_errors, list):\n",
    "    for i, error in enumerate(result_2.structured_errors, 1):\n",
    "        print(f\"\\nError {i}: {error.category}\")\n",
    "        print(f\"  Location: \\\"{error.error_occurrence}\\\"\")\n",
    "        print(f\"  Issue: {error.error}\")\n",
    "        print(f\"  Reasoning: {error.reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 3: Medication Question - Correct Answer\n",
    "\n",
    "**Scenario**: AI correctly answers a medication question (should have no errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference (Patient Question):\n",
      "\n",
      "Can I take ibuprofen if I'm on blood thinners?\n",
      "\n",
      "\n",
      "Candidate (AI Answer):\n",
      "\n",
      "You should avoid taking ibuprofen while on blood thinners. Ibuprofen is an NSAID \n",
      "that can increase bleeding risk when combined with anticoagulants. Please consult \n",
      "your doctor about safer pain relief alternatives like acetaminophen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reference_3 = \"\"\"\n",
    "Can I take ibuprofen if I'm on blood thinners?\n",
    "\"\"\"\n",
    "\n",
    "candidate_3 = \"\"\"\n",
    "You should avoid taking ibuprofen while on blood thinners. Ibuprofen is an NSAID \n",
    "that can increase bleeding risk when combined with anticoagulants. Please consult \n",
    "your doctor about safer pain relief alternatives like acetaminophen.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Reference (Patient Question):\")\n",
    "print(reference_3)\n",
    "print(\"\\nCandidate (AI Answer):\")\n",
    "print(candidate_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected task: medication2answer...\n",
      "\n",
      "Risk Level: 1/4\n",
      "Errors Found: 0\n",
      "\n",
      "No errors detected - AI response is accurate!\n"
     ]
    }
   ],
   "source": [
    "detected_3 = task_detector(reference=reference_3, candidate=candidate_3)\n",
    "print(f\"Detected task: {detected_3.task}...\\n\")\n",
    "\n",
    "instructions = task_prompts.get(detected_3.task, \"\")\n",
    "\n",
    "result_3 = validator(\n",
    "    instruction=instructions,\n",
    "    reference=reference_3,\n",
    "    candidate=candidate_3\n",
    ")\n",
    "\n",
    "print(f\"Risk Level: {result_3.risk_level}/4\")\n",
    "print(f\"Errors Found: {len(result_3.structured_errors)}\")\n",
    "\n",
    "if len(result_3.structured_errors) == 0:\n",
    "    print(\"\\nNo errors detected - AI response is accurate!\")\n",
    "else:\n",
    "    print(\"\\nErrors detected:\")\n",
    "    for error in result_3.structured_errors:\n",
    "        print(f\"  - {error.category}: {error.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Complete Workflow Function\n",
    "\n",
    "Create a simple function that physicians can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Function defined\n"
     ]
    }
   ],
   "source": [
    "def validate_medical_text(reference: str, candidate: str, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Complete validation workflow for physicians.\n",
    "    \n",
    "    Args:\n",
    "        reference: Original medical text (doctor's notes, report, etc.)\n",
    "        candidate: AI-generated output to validate\n",
    "        verbose: Print detailed output\n",
    "    \n",
    "    Returns:\n",
    "        dict with validation results\n",
    "    \"\"\"\n",
    "    # detect task\n",
    "    if verbose:\n",
    "        print(\"Step 1: Detecting task instruction...\")\n",
    "    \n",
    "    detected = task_detector(reference=reference, candidate=candidate)\n",
    "    \n",
    "    instruction = task_prompts.get(detected.task, \"\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"✓ Detected: {instruction[:100]}...\\n\")\n",
    "    \n",
    "    # validate\n",
    "    if verbose:\n",
    "        print(\"Step 2: Validating candidate against reference...\")\n",
    "    \n",
    "    result = validator(\n",
    "        instruction=instruction,\n",
    "        reference=reference,\n",
    "        candidate=candidate\n",
    "    )\n",
    "    \n",
    "    # format results\n",
    "    validation_result = {\n",
    "        \"detected_instruction\": instruction,\n",
    "        \"overall_risk_level\": result.risk_level,\n",
    "        \"errors\": result.structured_errors,\n",
    "        \"num_errors\": len(result.structured_errors) if isinstance(result.structured_errors, list) else 0\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"✓ Validation complete\\n\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"VALIDATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Overall Risk Level: {result.risk_level}/4\")\n",
    "        print(f\"Errors Found: {validation_result['num_errors']}\\n\")\n",
    "        \n",
    "        if isinstance(result.structured_errors, list) and len(result.structured_errors) > 0:\n",
    "            for i, error in enumerate(result.structured_errors, 1):\n",
    "                print(f\"Error {i}:\")\n",
    "                print(f\"Category: {error.category}\")\n",
    "                print(f\"Location: \\\"{error.error_occurrence[:100]}...\\\"\")\n",
    "                print(f\"Issue: {error.error}\")\n",
    "                print(f\"Reasoning: {error.reasoning[:200]}...\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"No errors detected!\")\n",
    "    \n",
    "    return validation_result\n",
    "\n",
    "print(\"✓ Function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Complete Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Detecting task instruction...\n",
      "✓ Detected: Create a simplified, patient-friendly version of the reference.\n",
      "1. Reference Description: The origin...\n",
      "\n",
      "Step 2: Validating candidate against reference...\n",
      "✓ Validation complete\n",
      "\n",
      "============================================================\n",
      "VALIDATION RESULTS\n",
      "============================================================\n",
      "Overall Risk Level: 4/4\n",
      "Errors Found: 2\n",
      "\n",
      "Error 1:\n",
      "Category: Understating intensity\n",
      "Location: \"Your appendix looks slightly swollen on the scan, but nothing to worry about....\"\n",
      "Issue: Downplaying severity of appendicitis\n",
      "Reasoning: The original reference indicates acute appendicitis, which is a medical emergency requiring immediate attention, contrary to the candidate's reassurance....\n",
      "\n",
      "Error 2:\n",
      "Category: Incorrect recommendation\n",
      "Location: \"You should schedule a routine follow-up appointment with your doctor next week to discuss monitoring...\"\n",
      "Issue: Incorrect recommendation for routine follow-up instead of urgent surgery.\n",
      "Reasoning: The recommendation in the reference is for an urgent appendectomy within 24 hours, while the candidate suggests a delayed follow-up which is inappropriate for a diagnosis of acute appendicitis....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use Test Case 1 with the workflow function\n",
    "result = validate_medical_text(reference_1, candidate_1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results to JSON\n",
    "\n",
    "Show how structured errors can be exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Output:\n",
      "{\n",
      "  \"overall_risk_level\": 4,\n",
      "  \"num_errors\": 2,\n",
      "  \"errors\": [\n",
      "    {\n",
      "      \"error_occurrence\": \"Your appendix looks slightly swollen on the scan, but nothing to worry about.\",\n",
      "      \"error\": \"Downplaying severity of appendicitis\",\n",
      "      \"category\": \"Understating intensity\",\n",
      "      \"reasoning\": \"The original reference indicates acute appendicitis, which is a medical emergency requiring immediate attention, contrary to the candidate's reassurance.\"\n",
      "    },\n",
      "    {\n",
      "      \"error_occurrence\": \"You should schedule a routine follow-up appointment with your doctor next week to discuss monitoring options.\",\n",
      "      \"error\": \"Incorrect recommendation for routine follow-up instead of urgent surgery.\",\n",
      "      \"category\": \"Incorrect recommendation\",\n",
      "      \"reasoning\": \"The recommendation in the reference is for an urgent appendectomy within 24 hours, while the candidate suggests a delayed follow-up which is inappropriate for a diagnosis of acute appendicitis.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "✓ Saved to validation_result.json\n"
     ]
    }
   ],
   "source": [
    "if isinstance(result_1.structured_errors, list) and len(result_1.structured_errors) > 0:\n",
    "    # convert ErrorAssessment objects to dict\n",
    "    errors_dict = [\n",
    "        {\n",
    "            \"error_occurrence\": error.error_occurrence,\n",
    "            \"error\": error.error,\n",
    "            \"category\": error.category,\n",
    "            \"reasoning\": error.reasoning\n",
    "        }\n",
    "        for error in result_1.structured_errors\n",
    "    ]\n",
    "    \n",
    "    output = {\n",
    "        \"overall_risk_level\": result_1.risk_level,\n",
    "        \"num_errors\": len(result_1.structured_errors),\n",
    "        \"errors\": errors_dict\n",
    "    }\n",
    "    \n",
    "    print(\"JSON Output:\")\n",
    "    print(json.dumps(output, indent=2))\n",
    "    \n",
    "    # save to file\n",
    "    with open(\"validation_result.json\", \"w\") as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    print(\"\\n✓ Saved to validation_result.json\")\n",
    "else:\n",
    "    print(\"Errors not in expected format for JSON export\")\n",
    "    print(f\"Errors type: {type(result_1.structured_errors)}\")\n",
    "    print(f\"Errors content: {result_1.structured_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging: Check DSPy Pydantic Support\n",
    "\n",
    "If structured output isn't working, let's check what DSPy is actually returning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy Result Inspection:\n",
      "Result type: <class 'dspy.primitives.prediction.Prediction'>\n",
      "Result attributes: ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__float__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rtruediv__', '__setattr__', '__setitem__', '__sizeof__', '__static_attributes__', '__str__', '__subclasshook__', '__truediv__', '__weakref__', '_completions', '_lm_usage', '_store', 'completions', 'copy', 'from_completions', 'get', 'get_lm_usage', 'inputs', 'items', 'keys', 'labels', 'set_lm_usage', 'toDict', 'values', 'with_inputs', 'without']\n",
      "\n",
      "Errors field type: <class 'list'>\n",
      "Errors content preview: [ErrorAssessment(error_occurrence='Your appendix looks slightly swollen on the scan, but nothing to worry about.', error='Downplaying severity of appendicitis', category='Understating intensity', reasoning=\"The original reference indicates acute appendicitis, which is a medical emergency requiring immediate attention, contrary to the candidate's reassurance.\"), ErrorAssessment(error_occurrence='You should schedule a routine follow-up appointment with your doctor next week to discuss monitoring o\n",
      "\n",
      "DSPy version: 3.0.4\n"
     ]
    }
   ],
   "source": [
    "print(\"DSPy Result Inspection:\")\n",
    "print(f\"Result type: {type(result_1)}\")\n",
    "print(f\"Result attributes: {dir(result_1)}\")\n",
    "print(f\"\\nErrors field type: {type(result_1.structured_errors)}\")\n",
    "print(f\"Errors content preview: {str(result_1.structured_errors)[:500]}\")\n",
    "\n",
    "import dspy\n",
    "print(f\"\\nDSPy version: {dspy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What We Tested\n",
    "1. Auto-detection of task instruction from reference + candidate\n",
    "2. Validation using detected instruction\n",
    "3. Structured `ErrorAssessment` output (if supported by DSPy version)\n",
    "4. Complete physician workflow function\n",
    "5. JSON export of results\n",
    "\n",
    "### Key Findings\n",
    "- Check if `List[ErrorAssessment]` works or if DSPy returns strings\n",
    "- Verify task detection correctly identifies instruction from `task_values`\n",
    "- Confirm risk levels are accurate\n",
    "\n",
    "### If Pydantic Output Doesn't Work\n",
    "Two options:\n",
    "1. **Update DSPy** to version 2.5+ (supports Pydantic directly)\n",
    "2. **Parse string output** - Keep `errors: str` in signature and parse to Pydantic manually"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
